{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from lgg_model import *\n",
    "from gensim.models import Word2Vec\n",
    "# from translate import *\n",
    "\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vanilla_GRU(\n",
       "  (Embedding): Embedding(1944, 50)\n",
       "  (GRU): GRU(50, 100, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (Linear): Linear(in_features=100, out_features=1944, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgg_model_path = input(\"è¯·è¾“å…¥æƒ³ä½¿ç”¨çš„è¯­è¨€æ¨¡å‹(æ— éœ€æ·»åŠ åç¼€)ï¼š\")\n",
    "lgg_model_path = 'lgg_model_paths/' + lgg_model_path\n",
    "lgg_model = torch.load(lgg_model_path, map_location='cpu')\n",
    "lgg_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model_path = input(\"è¯·ä½¿ç”¨æƒ³ä½¿ç”¨çš„è¯æ±‡åº“(æ— éœ€æ·»åŠ åç¼€)ï¼š\")\n",
    "word_model_path = 'word_model_paths/' + word_model_path\n",
    "word_model = Word2Vec.load(word_model_path)\n",
    "wv = word_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, GPT2LMHeadModel\n",
    "tokenizer = BertTokenizer.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -9.9010,  -9.7920, -10.0020,  ...,  -9.7487,  -9.9116,  -9.7439],\n",
       "        [ -3.2599,  -2.0971,  -2.6715,  ...,  -2.8029,  -3.2739,  -2.6459],\n",
       "        [ -8.4065,  -7.5731,  -7.3951,  ...,  -8.3534,  -8.0513,  -8.2858],\n",
       "        [-12.6778, -13.7086, -11.7202,  ..., -12.8463, -13.3368, -12.7257]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgg_model(torch.tensor([101, 10000, 22, 102]))['logits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21128"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é»„é›¾å¸¦ä¸‡å¥½äº†ä¸€æ¬¡ç›´ç”·çš„ä¸€èˆ¬å·¨Daveä¹Ÿæœ‰ç‚¹æ‰¾å›æ‰€è°“  åŒå¿ƒå¿ƒç¢\n",
      "[æ´ä¸»] 1æƒ³å››ä¸ªå…³ç³»çš„ä¸‰è¡Œé™ï¼Œä¸ç»ˆå¤œè¿…çª¥ï¼Œä¼¼ä¹æœ‰ç½¢èº«çˆ½ åˆ†æ‹¼ä¸æˆä½œæˆ‘åŠ¨äº†å¯èƒ½æ˜¯è‡ªå·±å…³ç³»åŒ—äº¬äº†ï¼‰ï¼Œåªæœ‰æ‰€ä»¥ä¸ä¸ºæ–°æ–‡æœ‰äººäº†ï¼Œä¸å¦‚è¿…é€Ÿæ‰©æœ›å‡‘ç™¾é¡¿ï¼Œæ—©ä¸Šæ‰¾å»ä»–çš„æœºè¿˜å¯ä»¥å»è¯´è¯ã€‚ç„¶è¯´è°ˆæ‹çˆ± Gå›ä½ æ‹çˆ±ä»–è¯´ç«‹ç‚¹æ¥è§¦ï¼Œå’Œå¥¹ä»¬å…¶å®ä¸æ˜¯ï¼Œä½ çš„é‚£ä¸€å¤©ï¼Œåœ¨é‚£ä¹ˆçš„é‚£æ–¹ä½\n",
      "[Magic Carol] è¯´é‡æ–°å®šä¹‰æ­£ï¼Œè¿˜æ˜¯æ˜¾ç„¶æ²¡æœ‰æœ€ç»ˆé™·å…¥çœ¼è‰²æ„Ÿè§‰ï¼Œä½†æˆ‘ä»¬å‚»è¦æè¿‡å‡ æ—¶ï¼Œéƒ½ç®—åˆ†è§¦è€ƒç¬‘çš„å¤´ç¥æ¡è®¾å…‰ä¸‹èµ·ä¸Šï¼Œä»æ­¤å°±æ˜¯æ›´ç¨³å®šæœ‰äººè¿™ä»€ä¹ˆä¸ºå‘Šé€€ï¼Œç„¶åæ¥æ— äººä¼šå®‰æ’åŠ¨äº†ï¼Œç„¶åéš¾è§‰çš„ è¯¥åŒé€‚æƒ…ä¸ä¼šï¼ŒçœŸçš„æ˜¯ä¸ºT\n",
      "æƒ³å’Œæˆ‘è¦æ”¯æŒä¸‹å§\n",
      "[Crazy Bob] ç•™å\n",
      "[Baby Yasmine] æ’å¿ƒå¿«é€Ÿç¢\n",
      "[Greedy Jason] Re æ´ä¸»: å“¥è¡—å–„æˆ‘äº†\n",
      "[Alice] æ—©å¤œæœ‰å¤æ»‘\n",
      "[æ´ä¸»] 0281114760 ç¥–ä½•æ—¶é²œçš„è§’åŠ¨ã€‚\n",
      "[Susan] äº†Aliceæ²‰çº¢çš„çš®è‚¤æ³›ã€‚çš®è‚¤æ³›è¯¾é‚£å¤´å–æ±Ÿæœ€åä¸€é¢ä¸Šé“ºï¼Œæœç»å¸¸å¥½å¯æ€»æ˜¯æˆ‘å°±å¥½å¥½çš„æ—¶å€™çš„æ—¶é—´è¦å»ç›®è§¦ä¼šï¼Œä¸å¥½é”€æ¯ã€‚\n",
      "å¯ä»¥åŠ å…¥ç²¾é¡¶å‹æŠ‘æ°›å›´åˆ©ï¼Œå¼•å¯¼é£åˆå½’åäº†äººã€è¾ˆã€‚é‚£ä¸ªé²œå­ç›¸ä¼šç©ºæ°”è¸è€Œä¸ä½ã€‚â€æ—‹å³ç»½æ‰‹å®¶å›è·¯ä¼šDaveæ­£äº†å‡ºåœºç‰‡æœªåã€‚\n",
      "[Dave] Re Angry Dave: å¹¶æ–‡â™€ å§¬ä»”ä¸å¤è¿”\n",
      "[Magic Alice] èµ¶å›¾åŸæ¥æ˜¯ä¸ªå‹ºé›†ç”¨è¯´äººï¼Œæ•™çš„å®¤å‹é‚£äº›ã€‚ï¼Œå›å¤æƒ…ï¼Œå½“å³ç»´åæŠŠDaveå› ä¸ºä»–ä¹Ÿè®¸ç»™ä»–\n",
      "[æ´ä¸»] æ‚¬é†’è¢«éªšæ‰°å’ŒAliceä¹Ÿå»è‡ªå·±å¿ƒä¸­æ„ä¹‰ï½\n",
      "[æ´ä¸»] è¿˜è¦æ•´ç¡\n",
      "[æ´ä¸»] 12893741çš„287æ¥¼â™€ è¿™ä¹ˆè‡ªç„¶ä¸€èµ·ï¼Œä½ æ­£å¥½åƒæ˜¯æ™¾çŠ¶æ€ã€‚Bobä¹Ÿæ­£å­æ–™å’Œä¹‹å‰ä¸æ¸…æ¥šä¸ºä¸ºä»€ä¹ˆç†„æˆåŠŸï¼Œä½†æ˜¯ä»–æƒ³å¼€å£æ°”ï¼Œæ€¥è¡£å…¨çš„æƒŠäººåŠ³ç´¯ã€‚\n",
      "è¿˜æ˜¯æˆ‘ä»¬è‡ªå·±çš„ä»€ä¹ˆä¹Ÿä¸è¿‡ä»–è¿½ä»€ä¹ˆå‘¢ï¼Ÿå›é¿å‹ä¾æ‹çˆ±ï¼Œè‚¯å®šè¿™ä¹ˆå¤šæ„ä¹‰çš„ä¸çˆ½çš„è‚©è†€åˆšåˆšåˆšæŠ¬çŸ¥é“\n",
      "[Angry Isabella] Re : é«˜æ•°å˜\n",
      "[Excited Carol] \n",
      "ç‹å®¶å†›æ‹“æ‰‘ï¼Œè¯—è¯¾Aliceã€Bobå¥‰ä¸Šå¹¸ç¦ï¼Œä¸çŸ¥é“æˆ‘å¯ä»¥å•æ‰€ğŸ˜…ã€‚\n",
      "[Bob] Re Dave: å–é›¾åŠå¯’å‡ï¼Œå½“æ´ä¸ºä¹‹ä¹‹ï¼Œé—®45ä¹™æ¥¼é“è„‘åŒæ³›ï¼Œå¾…ç„¶è¿™æ—¶ä¹Ÿå¿˜äº†ä½ è°ˆæˆ‘ç”Ÿäº†ï¼Œæˆ‘ä¸€ç›´ä¹Ÿä¸ä¼šæ‹“æ‰‘åˆ†ï¼Œçœ‹æ­¤å‰æ¥ã€‚å½“æ¥ä¸‹ä¸€ä»¶å°±æ˜¯æ²¡æœ‰æ„æ€ï¼Œä»–ä»¬ä¹‹é—´å®è´µå‘€\n",
      "[Kind Xander] Re Baby Carol: è°¢è°¢aå›ï½\n",
      "[Margaret] æŠ¥å‘Šå­¦ä¹ ä¹°é†‰\n",
      "[Angry Isabella] Re Greedy Louis: å¥½å®¶ä¼™\n",
      "[Crazy\n",
      "Generation finished.\n"
     ]
    }
   ],
   "source": [
    "words = input(\"è¯·è¾“å…¥åˆå§‹æ–‡æœ¬ï¼š\")\n",
    "del_lst = []\n",
    "lst = list(words)\n",
    "\n",
    "for i in lst:\n",
    "    if i not in wv.key_to_index:\n",
    "        del_lst.append(i)\n",
    "for i in del_lst:\n",
    "    lst.remove(i)\n",
    "\n",
    "data = np.array([])\n",
    "for i in lst:\n",
    "    data = np.append(data, wv.key_to_index[i])\n",
    "\n",
    "count = int(input(\"è¯·è¾“å…¥æƒ³è¦ç”Ÿæˆçš„å­—(è¯)æ•°ï¼š\"))\n",
    "\n",
    "for i in lst:\n",
    "    print(i, end='')\n",
    "\n",
    "for i in range(count):\n",
    "    data = np.stack((data,))\n",
    "    x = torch.Tensor(data)\n",
    "    x = x.to(torch.long)\n",
    "    y = lgg_model(x)[0][-1]\n",
    "    p = y.detach().numpy()\n",
    "    p = softmax(p)\n",
    "\n",
    "    idx = np.random.choice(np.arange(len(wv)), p=p)\n",
    "    new_word = wv.index_to_key[idx]\n",
    "    print(new_word, end='')\n",
    "\n",
    "    lst.append(new_word)\n",
    "    data = np.append(data, idx)\n",
    "\n",
    "print('\\nGeneration finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 101, 1506, 1506, 1506,  102])\n",
      "tensor([-15.1963, -15.3544, -15.0870,  ..., -15.3492, -15.1792, -14.7699],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "å“ˆå“ˆå“ˆ[[ U N K ]ob][ S E P ][[ U N K ]ob][ S E P ][ U N K ]azæ›´æ–°å®šç†è§£çŸ›ç›¾çš„kfrazï¼Œæ—©æ—¥å¸¸æ±‰è¯­æ•´å¾—æ²»æ„ˆè‡ªå·±é¢“åºŸ[ S E P ][[ U N K ][ U N K ]å›ä¸€å®šè§£çŸ›ç›¾çš„æ•…æ„éš¾å—å®³æ€•æ­¤å‰æ¥äº‰å»æ•™ï¼Œæˆ‘[ U N K ]æˆ‘ä¸æ‡‚å¾—æ‡‚å¾—ç‹¬è‡ªå·±è¦æ±‚"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/kuangyuxuan/Desktop/22Spring/Intro2AI/NLP_/Intro2AI_NLP/passage_generator/generate.ipynb Cell 8'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kuangyuxuan/Desktop/22Spring/Intro2AI/NLP_/Intro2AI_NLP/passage_generator/generate.ipynb#ch0000005?line=17'>18</a>\u001b[0m     \u001b[39mprint\u001b[39m(i, end\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kuangyuxuan/Desktop/22Spring/Intro2AI/NLP_/Intro2AI_NLP/passage_generator/generate.ipynb#ch0000005?line=19'>20</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(count):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/kuangyuxuan/Desktop/22Spring/Intro2AI/NLP_/Intro2AI_NLP/passage_generator/generate.ipynb#ch0000005?line=20'>21</a>\u001b[0m     y \u001b[39m=\u001b[39m lgg_model(data)[\u001b[39m0\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kuangyuxuan/Desktop/22Spring/Intro2AI/NLP_/Intro2AI_NLP/passage_generator/generate.ipynb#ch0000005?line=21'>22</a>\u001b[0m     p \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mdetach()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kuangyuxuan/Desktop/22Spring/Intro2AI/NLP_/Intro2AI_NLP/passage_generator/generate.ipynb#ch0000005?line=22'>23</a>\u001b[0m     p \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msoftmax(p, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:1047\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1038'>1039</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1039'>1040</a>\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1040'>1041</a>\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1041'>1042</a>\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1042'>1043</a>\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1043'>1044</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1044'>1045</a>\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1046'>1047</a>\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1047'>1048</a>\u001b[0m     input_ids,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1048'>1049</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1049'>1050</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1050'>1051</a>\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1051'>1052</a>\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1052'>1053</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1053'>1054</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1054'>1055</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1055'>1056</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1056'>1057</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1057'>1058</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1058'>1059</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1059'>1060</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1060'>1061</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1061'>1062</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1063'>1064</a>\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:890\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=879'>880</a>\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=880'>881</a>\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=881'>882</a>\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=886'>887</a>\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=887'>888</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=888'>889</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=889'>890</a>\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=890'>891</a>\u001b[0m         hidden_states,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=891'>892</a>\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=892'>893</a>\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=893'>894</a>\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=894'>895</a>\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=895'>896</a>\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=896'>897</a>\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=897'>898</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=898'>899</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=900'>901</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=901'>902</a>\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:432\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=429'>430</a>\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=430'>431</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_2(hidden_states)\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=431'>432</a>\u001b[0m feed_forward_hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(hidden_states)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=432'>433</a>\u001b[0m \u001b[39m# residual connection\u001b[39;00m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=433'>434</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m feed_forward_hidden_states\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:361\u001b[0m, in \u001b[0;36mGPT2MLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=358'>359</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_fc(hidden_states)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=359'>360</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact(hidden_states)\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=360'>361</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mc_proj(hidden_states)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=361'>362</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=362'>363</a>\u001b[0m \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/modeling_utils.py:1871\u001b[0m, in \u001b[0;36mConv1D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/modeling_utils.py?line=1868'>1869</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/modeling_utils.py?line=1869'>1870</a>\u001b[0m     size_out \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnf,)\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/modeling_utils.py?line=1870'>1871</a>\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49maddmm(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, x\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, x\u001b[39m.\u001b[39;49msize(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/modeling_utils.py?line=1871'>1872</a>\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(size_out)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/modeling_utils.py?line=1872'>1873</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"GPT-2 model generation\"\"\"\n",
    "words = input(\"è¯·è¾“å…¥åˆå§‹æ–‡æœ¬ï¼š\")\n",
    "del_lst = []\n",
    "lst = list(words)\n",
    "\n",
    "data = []\n",
    "data.append(101)\n",
    "for i in lst:\n",
    "    data.append(tokenizer.encode(i)[1])\n",
    "data.append(102)\n",
    "data = torch.tensor(data)\n",
    "print(data)\n",
    "print(lgg_model(data)[0][-1])\n",
    "\n",
    "count = int(input(\"è¯·è¾“å…¥æƒ³è¦ç”Ÿæˆçš„å­—(è¯)æ•°ï¼š\"))\n",
    "\n",
    "for i in lst:\n",
    "    print(i, end='')\n",
    "\n",
    "for i in range(count):\n",
    "    y = lgg_model(data)[0][-1]\n",
    "    p = y.detach()\n",
    "    p = torch.softmax(p, dim=0)\n",
    "\n",
    "    idx = np.random.choice(tokenizer.vocab_size, p=np.array(p))\n",
    "    new_word = tokenizer.decode(idx)\n",
    "    print(new_word, end='')\n",
    "\n",
    "    lst.append(new_word)\n",
    "    data = torch.hstack((data, torch.tensor([idx])))\n",
    "\n",
    "print('\\nGeneration finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate the generated text into classical Chinese\n",
    "to_be_translated = str(lst)\n",
    "translated = inference(to_be_translated)\n",
    "for i in translated[0]:\n",
    "    print(i, end='')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
